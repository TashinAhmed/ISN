{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import ssl\n",
    "import segmentation_models_pytorch as smp\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import multiprocessing as mp\n",
    "from torch import nn\n",
    "from segmentation_models_pytorch.encoders import get_preprocessing_fn\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "import pandas as pd\n",
    "from pytorch_lightning.loggers import CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODER = 'efficientnet-b2'\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "CHANNELS = 3\n",
    "CLASSES = 2\n",
    "ACTIVATION = 'sigmoid'\n",
    "LR = 0.001\n",
    "BATCH_SIZE = 8\n",
    "IMG_SIZE = (224,224)\n",
    "DEVICE, NUM_DEVICES = (\"cuda\", torch.cuda.device_count()) if torch.cuda.is_available() else (\"cpu\", mp.cpu_count())\n",
    "WORKERS = mp.cpu_count()\n",
    "print(f'Running on {NUM_DEVICES} {DEVICE}(s)')\n",
    "\n",
    "LR = 0.001\n",
    "EPOCHS = 10\n",
    "OUTPUT_DIR = \"./working\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data/preprocessed\"\n",
    "\n",
    "metadata_df = pd.read_csv(\"../data/preprocessed/metadata.csv\")\n",
    "metadata_df = metadata_df[metadata_df['split']=='experiment']\n",
    "metadata_df = metadata_df[['image_id', 'image_path', 'mask_path']]\n",
    "metadata_df['image_path'] = metadata_df['image_path'].apply(lambda img_pth: os.path.join(DATA_DIR, img_pth))\n",
    "metadata_df['mask_path'] = metadata_df['mask_path'].apply(lambda img_pth: os.path.join(DATA_DIR, img_pth))\n",
    "metadata_df = metadata_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "valid_df = metadata_df.sample(frac=0.1, random_state=42)\n",
    "minus_valid_df = metadata_df.drop(valid_df.index)\n",
    "test_df = minus_valid_df.sample(frac=0.1, random_state=42)\n",
    "train_df = minus_valid_df.drop(test_df.index)\n",
    "\n",
    "len(train_df), len(valid_df), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_df = pd.read_csv(os.path.join(DATA_DIR, 'class_dict.csv'))\n",
    "class_names = class_df['name'].tolist()\n",
    "class_rgb_values = class_df[['r','g','b']].values.tolist()\n",
    "\n",
    "print('All dataset classes and their corresponding RGB values in labels:')\n",
    "print('Class Names: ', class_names)\n",
    "print('Class RGB values: ', class_rgb_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(label, label_values):\n",
    "    semantic_map = []\n",
    "    for colour in label_values:\n",
    "        equality = np.equal(label, colour)\n",
    "        class_map = np.all(equality, axis = -1)\n",
    "        semantic_map.append(class_map)\n",
    "    semantic_map = np.stack(semantic_map, axis=-1)\n",
    "    return semantic_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationModel(pl.LightningModule):\n",
    "    def __init__(self, net, loss, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.lr = lr\n",
    "        self.loss = loss\n",
    "        self.net = net\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def shared_step(self, preds, labels):\n",
    "        tp, fp, fn, tn = smp.metrics.get_stats(preds, labels.long(), mode='binary', threshold=0.5)\n",
    "\n",
    "        iou_score = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n",
    "        f1_score = smp.metrics.f1_score(tp, fp, fn, tn, reduction=\"micro\")\n",
    "        accuracy = smp.metrics.accuracy(tp, fp, fn, tn, reduction=\"macro\")\n",
    "        recall = smp.metrics.recall(tp, fp, fn, tn, reduction=\"micro-imagewise\")\n",
    "\n",
    "        return {'iou_score': iou_score,\n",
    "                'f1_score': f1_score,\n",
    "                'accuracy': accuracy,\n",
    "                'recall': recall}\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        preds = self(imgs)\n",
    "\n",
    "        metrics = self.shared_step(preds, labels)\n",
    "\n",
    "        self.log('train_iou_score', metrics['iou_score'], on_epoch=True)     \n",
    "        self.log('train_f1_score', metrics['f1_score'], on_epoch=True)     \n",
    "        self.log('train_accuracy', metrics['accuracy'], on_epoch=True)     \n",
    "        self.log('train_recall', metrics['recall'], on_epoch=True)     \n",
    "\n",
    "        loss = self.loss(preds, labels)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss  \n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        preds = self(imgs)\n",
    "\n",
    "        metrics = self.shared_step(preds, labels)\n",
    "\n",
    "        self.log('valid_iou_score', metrics['iou_score'], on_epoch=True)     \n",
    "        self.log('valid_f1_score', metrics['f1_score'], on_epoch=True)     \n",
    "        self.log('valid_accuracy', metrics['accuracy'], on_epoch=True)     \n",
    "        self.log('valid_recall', metrics['recall'], on_epoch=True)\n",
    "\n",
    "        loss = self.loss(preds, labels)\n",
    "        self.log(\"valid_loss\", loss)             \n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        preds = self(imgs)\n",
    "\n",
    "        metrics = self.shared_step(preds, labels)\n",
    "\n",
    "        self.log('test_iou_score', metrics['iou_score'], on_epoch=True)     \n",
    "        self.log('test_f1_score', metrics['f1_score'], on_epoch=True)     \n",
    "        self.log('test_accuracy', metrics['accuracy'], on_epoch=True)     \n",
    "        self.log('test_recall', metrics['recall'], on_epoch=True)\n",
    "\n",
    "        loss = self.loss(preds, labels)\n",
    "        self.log(\"test_loss\", loss) \n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.Adam(self.net.parameters(), lr=self.lr)\n",
    "        return [opt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ISNSet(Dataset):\n",
    "    def __init__(self, df, transform=None, preprocess_fn=None, class_rgb_values=None):\n",
    "        self.image_paths = df['image_path'].tolist()\n",
    "        self.mask_paths = df['mask_path'].tolist()\n",
    "        self.transform = transform\n",
    "        self.preprocess_fn = preprocess_fn\n",
    "        self.class_rgb_values = class_rgb_values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_file = self.image_paths[idx]\n",
    "        mask_file = self.mask_paths[idx]\n",
    "\n",
    "        img = Image.open(img_file).convert('RGB')\n",
    "        img = np.array(img)\n",
    "\n",
    "        mask = Image.open(mask_file).convert('RGB')\n",
    "        # one-hot-encode the mask\n",
    "        mask = one_hot_encode(mask, self.class_rgb_values).astype('float')\n",
    "\n",
    "        if self.preprocess_fn:\n",
    "            img = self.preprocess_fn(img)\n",
    "            img = np.array(img, dtype=np.float32)\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        return img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ISNDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_df, valid_df, test_df, batch_size=2, img_size=(256,256), preprocess_fn=None, class_rgb_values=None):\n",
    "        super().__init__()\n",
    "        self.train_df = train_df\n",
    "        self.valid_df = valid_df\n",
    "        self.test_df = test_df\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize(size=self.img_size),\n",
    "        ])\n",
    "        self.preprocess_fn = preprocess_fn\n",
    "        self.class_rgb_values = class_rgb_values\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.trainset = ISNSet(self.train_df, transform=self.transform, preprocess_fn=self.preprocess_fn, class_rgb_values=self.class_rgb_values)\n",
    "        self.validset = ISNSet(self.valid_df, transform=self.transform, preprocess_fn=self.preprocess_fn, class_rgb_values=self.class_rgb_values)\n",
    "        self.testset = ISNSet(self.test_df, transform=self.transform, preprocess_fn=self.preprocess_fn, class_rgb_values=self.class_rgb_values)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.trainset, batch_size=self.batch_size, shuffle=True, num_workers=WORKERS, pin_memory=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.validset, batch_size=self.batch_size, shuffle=False, num_workers=WORKERS, pin_memory=True)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.testset, batch_size=self.batch_size, shuffle=False, num_workers=WORKERS, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "net = smp.UnetPlusPlus(\n",
    "    encoder_name=ENCODER,                       \n",
    "    encoder_weights=ENCODER_WEIGHTS,            \n",
    "    in_channels=CHANNELS,                       \n",
    "    classes=CLASSES,                            \n",
    "    activation= ACTIVATION                  \n",
    ")\n",
    "\n",
    "preprocess_input = get_preprocessing_fn(ENCODER, pretrained=ENCODER_WEIGHTS)\n",
    "\n",
    "LOSS = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='valid_loss', \n",
    "    min_delta=0.00001, \n",
    "    patience=5, \n",
    "    mode='min')\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    every_n_epochs=1,\n",
    "    dirpath=OUTPUT_DIR,\n",
    "    filename='lightning_trained'\n",
    ")\n",
    "\n",
    "logger = CSVLogger(OUTPUT_DIR, name='lightning_logs')\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "        accelerator=DEVICE,\n",
    "        devices=NUM_DEVICES,\n",
    "        max_epochs=EPOCHS,\n",
    "        callbacks=[early_stop_callback, checkpoint_callback],\n",
    "        logger=logger,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmodel = SegmentationModel(net, LOSS, LR)\n",
    "isndata = ISNDataModule(train_df, valid_df, test_df, BATCH_SIZE, IMG_SIZE, preprocess_input, class_rgb_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = '../working/savedckpt/'\n",
    "checkpoint_file = \"../working/savedckpts/lightning_trained-v1.ckpt\"\n",
    "\n",
    "segmodel = SegmentationModel.load_from_checkpoint(checkpoint_file)\n",
    "\n",
    "isndata = ISNDataModule(train_df, valid_df, test_df, \n",
    "                                    BATCH_SIZE, IMG_SIZE, \n",
    "                                    preprocess_input, class_rgb_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_one_hot(image):\n",
    "    x = np.argmax(image, axis = 0)\n",
    "    return x\n",
    "\n",
    "\n",
    "def colour_code_segmentation(image, label_values):\n",
    "    colour_codes = np.array(label_values)\n",
    "    x = colour_codes[image.astype(int)]\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "# def reverse_one_hot(image):\n",
    "#     x = np.argmax(image, axis=-1)  # Ensure correct axis is used\n",
    "#     return x\n",
    "\n",
    "\n",
    "# def colour_code_segmentation(image, label_values):\n",
    "#     colour_codes = np.array(label_values)\n",
    "#     max_index = len(colour_codes) - 1\n",
    "#     # Clip indices to ensure they are within the valid range\n",
    "#     image = np.clip(image, 0, max_index)\n",
    "#     x = colour_codes[image.astype(int)]\n",
    "#     return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(**images):\n",
    "    \"\"\"PLot images in one row.\"\"\"\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(' '.join(name.split('_')).title())\n",
    "        plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(size=IMG_SIZE),\n",
    "])\n",
    "\n",
    "testset = ISNSet(test_df, transform=transform, preprocess_fn=preprocess_input, class_rgb_values=class_rgb_values)\n",
    "testsetvis = ISNSet(test_df, transform=transform, preprocess_fn=None, class_rgb_values=class_rgb_values)\n",
    "\n",
    "reverse_transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize(size=IMG_SIZE),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "# Ensure the device is correctly configured\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Helper function to visualize and save the figures\n",
    "def visualize_and_save(image, ground_truth_mask, prediction, idx):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 10))\n",
    "    ax = axes.ravel()\n",
    "    \n",
    "    ax[0].imshow(image)\n",
    "    ax[0].set_title('Original Image')\n",
    "    \n",
    "    ax[1].imshow(ground_truth_mask)\n",
    "    ax[1].set_title('Ground Truth')\n",
    "    \n",
    "    ax[2].imshow(prediction)\n",
    "    ax[2].set_title('Prediction')\n",
    "    \n",
    "    for a in ax:\n",
    "        a.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure instead of showing it\n",
    "    plt.savefig(f'output_figure_{idx}.png')\n",
    "    \n",
    "    # Close the figure to avoid memory leaks\n",
    "    plt.close(fig)\n",
    "\n",
    "for i in range(30):\n",
    "    n = np.random.choice(len(testset))\n",
    "    \n",
    "    image, gt_mask = testset[n]\n",
    "    imagevis, _ = testsetvis[n]\n",
    "    \n",
    "    # Move image to the device\n",
    "    image = image.unsqueeze(0).to(device)  \n",
    "    \n",
    "    # Ensure model is on the device\n",
    "    segmodel.to(device)\n",
    "    segmodel.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    # Perform prediction\n",
    "    with torch.no_grad():\n",
    "        pred = segmodel(image)\n",
    "    \n",
    "    # Move prediction back to CPU\n",
    "    pred = pred.squeeze(0).cpu().detach().numpy()\n",
    "    \n",
    "    gt_mask = gt_mask.permute(1, 2, 0).cpu().numpy().astype(np.uint8)\n",
    "    \n",
    "    reversed_gt_mask = reverse_one_hot(gt_mask)\n",
    "    reversed_pred = np.argmax(pred, axis=0)  \n",
    "    \n",
    "    ground_truth_mask = colour_code_segmentation(reversed_gt_mask, class_rgb_values).astype(np.uint8)\n",
    "    prediction = colour_code_segmentation(reversed_pred, class_rgb_values).astype(np.uint8)\n",
    "    \n",
    "    image_pil = transforms.ToPILImage()(imagevis.cpu())\n",
    "    \n",
    "    # Save the figure\n",
    "    visualize_and_save(\n",
    "        image=image_pil,\n",
    "        ground_truth_mask=ground_truth_mask,\n",
    "        prediction=prediction,\n",
    "        idx=i\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
